# JusCash Case ‚Äì Automa√ß√£o e Gerenciamento de Publica√ß√µes do DJE

Este reposit√≥rio monolito integra tr√™s servi√ßos principais para automatizar a coleta, gerenciamento e visualiza√ß√£o de publica√ß√µes do Di√°rio da Justi√ßa Eletr√¥nico (DJE) de S√£o Paulo:

- **backend-api**: API em Node.js + Express para CRUD, filtros, autentica√ß√£o e atualiza√ß√£o de status das publica√ß√µes.
- **juscash-frontend**: SPA React (Vite) com interface Kanban, telas de login/cadastro e modal de detalhes das publica√ß√µes.
- **tjsp-scraper**: Scraper em Python com SQLAlchemy para extrair dados do DJE e popular o banco.

Todos os servi√ßos s√£o orquestrados via Docker Compose junto com um banco PostgreSQL.

---

## üìÇ Estrutura do Reposit√≥rio

```text
/ (raiz)
‚îú‚îÄ‚îÄ backend-api/       # API Node.js + Express
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile     # Imagem Docker da API
‚îÇ   ‚îî‚îÄ‚îÄ src/‚Ä¶          # C√≥digo-fonte da API
‚îÇ
‚îú‚îÄ‚îÄ juscash-frontend/  # Frontend React (Vite)
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile     # Imagem Docker do Frontend
‚îÇ   ‚îî‚îÄ‚îÄ src/‚Ä¶          # C√≥digo-fonte do React
‚îÇ
‚îú‚îÄ‚îÄ tjsp-scraper/      # Scraper Python + SQLAlchemy
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile     # Imagem Docker do Scraper
‚îÇ   ‚îî‚îÄ‚îÄ src/‚Ä¶          # C√≥digo-fonte do Scraper
‚îÇ
‚îú‚îÄ‚îÄ docs/              # Documenta√ß√£o em PDF e diagramas
‚îú‚îÄ‚îÄ docker-compose.yml # Orquestra√ß√£o de containers
‚îú‚îÄ‚îÄ .gitignore         # Regras de ignore globais
‚îú‚îÄ‚îÄ .env.example       # Exemplo de vari√°veis de ambiente centralizado
‚îî‚îÄ‚îÄ README.md          # Este arquivo
```

---

## üîß Clonando o Reposit√≥rio

Para obter o c√≥digo localmente:

```bash
git clone https://github.com/nogmois/juscash-case.git
cd juscash-case
```

---

## üöÄ Pr√©-requisitos

- **Docker** (>= 20.x)
- **Docker Compose** (>= 1.29.x)

> **N√£o √© necess√°rio** ter Node.js, Python ou PostgreSQL instalados localmente; tudo roda em containers.

---

## ‚öôÔ∏è Configura√ß√£o de Ambiente

Todas as vari√°veis de ambiente est√£o centralizadas em um √∫nico arquivo na raiz do projeto. Crie o `.env` copiando e adequando o exemplo:

```bash
# Linux/macOS
to create the env file
cp .env.example .env

# Windows CMD
to create the env file
copy .env.example .env
```

Edite o `.env` para incluir todos os par√¢metros necess√°rios:

```dotenv
# Banco de Dados (PostgreSQL)
DB_HOST=db
DB_PORT=5432
DB_NAME=juscash_db
DB_USER=postgres
DB_PASS=postgres

# JWT
JWT_SECRET=seu_jwt_secret_aqui

# Frontend
VITE_API_URL=http://localhost:3000

# Scraper
DATABASE_URL=postgresql://postgres:postgres@db:5432/juscash_db
```

O Docker Compose carregar√° automaticamente este arquivo e injetar√° as vari√°veis em cada servi√ßo.

---

## ‚ñ∂Ô∏è Executando em Desenvolvimento

No diret√≥rio raiz, execute:

```bash
docker-compose up --build
```

Este comando ir√°:

1. Criar um container PostgreSQL (porta **5432**)
2. Buildar e iniciar a API (porta **8012**)
3. Buildar e iniciar o Frontend (porta **5173**)
4. Buildar e executar o Scraper (coleta inicial) e, em seguida, permanece rodando para agendamentos

A primeira execu√ß√£o do scraper popula o banco; coletas seguintes s√£o disparadas automaticamente conforme o cron interno.

Para parar e remover containers:

```bash
docker-compose down
```

---

## üîó Endpoints Principais

### API (Node.js + Express)

Base URL: `http://localhost:8012`

- **GET** `/api/publications` ‚Äî Lista todas as publica√ß√µes (suporta filtros: processo, data, status)
- **PATCH** `/api/publications/:id/status` ‚Äî Atualiza status (`new`, `read`, `sent_adv`, `done`)
- **Swagger UI**: `http://localhost:8012/api/docs`

### Frontend (React)

Base URL: `http://localhost:5173`

- Tela de Login e Cadastro
- Kanban para gerenciar publica√ß√µes por status
- Modal de detalhes com informa√ß√µes completas de cada publica√ß√£o

---

## üìù Documenta√ß√£o

Na pasta `docs/` voc√™ encontra:

- **Manual do Produto** (PDF): Vis√£o funcional e fluxo de uso para usu√°rios finais.
- **Documenta√ß√£o T√©cnica** (PDF):

  - Especifica√ß√£o OpenAPI/Swagger
  - Diagrama de banco de dados
  - Fluxos de scraping e automa√ß√£o
  - Instru√ß√µes de deploy
